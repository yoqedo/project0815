ðŸ“˜ Proxmox Netzwerkâ€‘ & Storageâ€‘Design
Dokumentation fÃ¼r Infrastrukturâ€‘ und Clusterâ€‘Vorbereitung

1. Ãœberblick
Diese Seite beschreibt die empfohlene Architektur fÃ¼r eine Proxmoxâ€‘Umgebung mit:
â€¢ 	2Ã— NVMe 512â€¯GB (interner Storage)
â€¢ 	externem Storage (NFS, iSCSI oder Ceph)
â€¢ 	Bonding + Bridges + VLANâ€‘Design
â€¢ 	Clusterâ€‘Vorbereitung
Die Struktur entspricht realistischen KMUâ€‘ und Enterpriseâ€‘Setups.

2. Interner Storage (2Ã— NVMe 512â€¯GB)
Installation (Proxmoxâ€‘Installer)
Die beiden NVMeâ€‘Disks werden als:
â€¢ 	ZFS RAID1 konfiguriert
â€¢ 	Poolâ€‘Name: 
Ergebnis der Installation

Hinweis:
Das OSâ€‘RAID wird nur im Installer erstellt, nicht in der GUI.

3. Externes Storage
HÃ¤ufig genutzte Varianten
A) NFS (TrueNAS, Synology, QNAP)
â€¢ 	am weitesten verbreitet
â€¢ 	einfach einzubinden
â€¢ 	ideal fÃ¼r VMâ€‘Disks
â€¢ 	ideal fÃ¼r Clusterâ€‘Sharedâ€‘Storage
â€¢ 	benÃ¶tigt 10Gbit fÃ¼r gute Performance
B) iSCSI (SANâ€‘Ã¤hnlich)
â€¢ 	blockbasiert
â€¢ 	sehr schnell
â€¢ 	ideal fÃ¼r LVMâ€‘Thinâ€‘Storage
â€¢ 	hÃ¤ufig in professionellen Umgebungen
C) Ceph (fÃ¼r echte Cluster)
â€¢ 	verteilt
â€¢ 	hochverfÃ¼gbar
â€¢ 	selbstheilend
â€¢ 	benÃ¶tigt mindestens 3 Nodes
â€¢ 	Enterpriseâ€‘Standard fÃ¼r HAâ€‘Cluster

4. Netzwerkdesign
Bonding (2Ã— 10Gbit)
Beide 10Gbitâ€‘Ports werden zu einem Bond (LACP) zusammengefasst.
Vorteile:
â€¢ 	Redundanz
â€¢ 	20Gbit Aggregation
â€¢ 	saubere Layerâ€‘2â€‘Topologie
â€¢ 	ideal fÃ¼r Storageâ€‘Traffic

5. Bridges & VLANâ€‘Design
Auf dem Bond werden mehrere Bridges erstellt.
Jede Bridge kann ein eigenes Netz/VLAN tragen und optional eine eigene IP erhalten.
Empfohlene Struktur

Wichtig
â€¢ 	vmbr0 ist nicht automatisch das Storageâ€‘Netz
â€¢ 	Storageâ€‘Traffic sollte immer getrennt vom Managementâ€‘Netz laufen
â€¢ 	Proxmox nutzt die IP der Bridge, nicht die physische NIC

6. Externes Storage anbinden (GUI)
Pfad:
Datacenter â†’ Storage â†’ Add
Beispiele:
NFS
â€¢ 	Server: 
â€¢ 	Export: 
â€¢ 	Content: Disk image, Container, ISO, Backup
â€¢ 	Nodes: alle Nodes
iSCSI
â€¢ 	Target: 
â€¢ 	Content: Disk image
â€¢ 	LUNs: automatisch

7. ZusÃ¤tzliche Disks (z.â€¯B. 6 weitere NVMe)
Proxmox erkennt alle Disks automatisch.
RAIDâ€‘Konfiguration in der GUI:
Node â†’ Disks â†’ ZFS â†’ Create: ZFS
MÃ¶gliche RAIDâ€‘Level:
â€¢ 	RAID10
â€¢ 	RAIDZ1
â€¢ 	RAIDZ2
â€¢ 	RAIDZ3
Best Practice:
OSâ€‘Disks getrennt lassen â†’ zusÃ¤tzliche Disks fÃ¼r VMâ€‘Storage nutzen.

8. Bestâ€‘Practice Architektur
Interne NVMe (2Ã— 512â€¯GB):
â€¢ 	ZFS RAID1 fÃ¼r OS
â€¢ 	lokaler Storage fÃ¼r ISOs, Templates, Backups
Externes Storage (NFS/iSCSI):
â€¢ 	VMâ€‘Disks
â€¢ 	Shared Storage fÃ¼r Cluster
â€¢ 	HAâ€‘fÃ¤hig
â€¢ 	Liveâ€‘Migration mÃ¶glich
Netzwerk:
â€¢ 	Bond0 (2Ã— 10Gbit)
â€¢ 	mehrere Bridges fÃ¼r Management, Storage, VMâ€‘Netz, Backup

9. Fazit
Diese Architektur ist:
â€¢ 	realistisch
â€¢ 	skalierbar
â€¢ 	clusterfÃ¤hig
â€¢ 	dokumentierbar
â€¢ 	ideal fÃ¼r Proxmoxâ€‘Labs und produktive KMUâ€‘Umgebungen
Sie bildet die Grundlage fÃ¼r:
â€¢ 	HAâ€‘Cluster
â€¢ 	Cephâ€‘Storage
â€¢ 	PBSâ€‘Backups
â€¢ 	Multiâ€‘Nodeâ€‘Infrastrukturen
