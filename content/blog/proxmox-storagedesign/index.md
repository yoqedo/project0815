

# üìò Proxmox Netzwerk- & Storage-Design
*Dokumentation f√ºr Infrastruktur- und Cluster-Vorbereitung*

---

## 1. √úberblick

Diese Dokumentation beschreibt die empfohlene Architektur f√ºr eine Proxmox-Umgebung mit:

- 2√ó NVMe 512‚ÄØGB (interner Storage)
- externem Storage (NFS, iSCSI oder Ceph)
- Bonding + Bridges + VLAN-Design
- Cluster-Vorbereitung

---

## 2. Interner Storage (2√ó NVMe 512‚ÄØGB)

### Installation (Proxmox-Installer)

Die beiden NVMe-Disks werden als:

- ZFS RAID1 konfiguriert
- Pool-Name: `rpool`

### Ergebnis der Installation

| Dataset | Zweck |
|--------|--------|
| `rpool/ROOT/pve-1` | Proxmox Betriebssystem |
| `rpool/data` | lokaler VM-Storage (optional) |
| `/var/lib/vz` | ISOs, Templates, Backups |

**Hinweis:**  
Das OS-RAID wird nur im Installer erstellt, nicht in der GUI.

---

## 3. Externes Storage

### H√§ufig genutzte Varianten

#### A) NFS (TrueNAS, Synology, QNAP)

- am weitesten verbreitet
- einfach einzubinden
- ideal f√ºr VM-Disks
- ideal f√ºr Cluster-Shared-Storage
- ben√∂tigt 10Gbit f√ºr gute Performance

#### B) iSCSI (SAN-√§hnlich)

- blockbasiert
- sehr schnell
- ideal f√ºr LVM-Thin-Storage
- h√§ufig in professionellen Umgebungen

#### C) Ceph (f√ºr echte Cluster)

- verteilt
- hochverf√ºgbar
- selbstheilend
- ben√∂tigt mindestens 3 Nodes
- Enterprise-Standard f√ºr HA-Cluster

---

## 4. Netzwerkdesign

### Bonding (2√ó 10Gbit)

Beide 10Gbit-Ports werden zu einem Bond (LACP) zusammengefasst.

Vorteile:

- Redundanz
- 20Gbit Aggregation
- saubere Layer-2-Topologie
- ideal f√ºr Storage-Traffic

---

## 5. Bridges & VLAN-Design

Auf dem Bond werden mehrere Bridges erstellt.  
Jede Bridge kann ein eigenes Netz/VLAN tragen und optional eine eigene IP erhalten.

### Empfohlene Struktur

| Bridge | Zweck | IP | VLAN |
|--------|--------|------|------|
| vmbr0 | Management | 192.168.x.x | mgmt-vlan |
| vmbr1 | Storage | 10.10.x.x | storage-vlan |
| vmbr2 | VM-Netz | keine IP | vm-vlan |
| vmbr3 | Backup | 10.20.x.x | backup-vlan |

### Wichtig

- vmbr0 ist nicht automatisch das Storage-Netz
- Storage-Traffic sollte immer getrennt vom Management-Netz laufen
- Proxmox nutzt die IP der Bridge, nicht die physische NIC

---

## 6. Externes Storage anbinden (GUI)

### Pfad:

```
Datacenter ‚Üí Storage ‚Üí Add
```

### Beispiele

#### NFS

- Server: `10.10.20.20`
- Export: `/mnt/vmstore`
- Content: Disk image, Container, ISO, Backup
- Nodes: alle Nodes

#### iSCSI

- Target: `iqn.2024-01.truenas:target1`
- Content: Disk image
- LUNs: automatisch

---

## 7. Zus√§tzliche Disks (z.‚ÄØB. 6 weitere NVMe)

Proxmox erkennt alle Disks automatisch.

### RAID-Konfiguration in der GUI

```
Node ‚Üí Disks ‚Üí ZFS ‚Üí Create: ZFS
```

M√∂gliche RAID-Level:

- RAID10
- RAIDZ1
- RAIDZ2
- RAIDZ3

**Best Practice:**  
OS-Disks getrennt lassen ‚Üí zus√§tzliche Disks f√ºr VM-Storage nutzen.

---

## 8. Best-Practice Architektur

### Interne NVMe (2√ó 512‚ÄØGB)

- ZFS RAID1 f√ºr OS
- lokaler Storage f√ºr ISOs, Templates, Backups

### Externes Storage (NFS/iSCSI)

- VM-Disks
- Shared Storage f√ºr Cluster
- HA-f√§hig
- Live-Migration m√∂glich

### Netzwerk

- Bond0 (2√ó 10Gbit)
- mehrere Bridges f√ºr Management, Storage, VM-Netz, Backup

---

## 9. Fazit

Diese Architektur ist:

- realistisch
- skalierbar
- clusterf√§hig
- dokumentierbar
- ideal f√ºr Proxmox-Labs und produktive KMU-Umgebungen

Sie bildet die Grundlage f√ºr:

- HA-Cluster
- Ceph-Storage
- PBS-Backups
- Multi-Node-Infrastrukturen
